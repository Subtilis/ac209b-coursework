---
title: |
  | Homework 4: SVMs & Return of the Bayes
author: "Tim Hagmann"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output:
  pdf_document:
    toc: yes
  html_document:
    css: css/styles.css
    highlight: tango
    theme: flatly
    toc: yes
    toc_float: yes
  word_document:
    toc: yes
subtitle: |
  | Advanced Topics in Data Science II
  | Harvard University, Spring 2017
affiliation: Harvard University
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo=TRUE)
```

```{r global_options, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE,
                      tidy.opts=list(width.cutoff=60), fig.pos='H',
                      fig.align='center')
```

# Problem 1: Celestial Object Classification

SVMs are computationally intensive, much more so than other methods we've used in the course. Expect run times for your analyses to be much larger than before. Several SVM packages are available, we recommend using the `e1071` library, though you're free to use whatever package you feel comfortable with -- we'll provide extra hints for the `svm` function from this package. 

In this problem, the task is to classify a celestial object into one of 4 categories using photometric measurements recorded about the object. The training and testing datasets are provided in the `dataset_1_train.txt` and `dataset_1_test.txt` respectively. Overall, there are a total of 1,379 celestial objects described by 61 attributes. The last column contains the object category we wish to predict, `Class`.

### Initialize
In the following code chunk all the necessary setup for the modelling environment is done.

```{r init, message=FALSE}
## Options
options(scipen = 10)                          # Disable scientific notation
update_package <- FALSE                       # Use old status of packages

## Init files (always execute, eta: 10s)
source("scripts/01_init.R")                   # Helper functions to load packages
source("scripts/02_packages.R")               # Load all necessary packages
source("scripts/03_functions.R")              # Load project specific functions
```

First, ensure that the that `Class` is a factor (quantitative values). These should be object categories and not integer values -- use `as.factor` if needed. 

### Load the data
```{r, message=FALSE}
## Read data
df_train <- data.frame(read_csv("data/dataset_1_train.txt"))
df_test <- data.frame(read_csv("data/dataset_1_test.txt"))
```

### Preprocess data
```{r}
# Transform y variable to factor
df_train$Class <- as.factor(df_train$Class)
df_test$Class <- as.factor(df_test$Class)
```

## 1. RBF Kernel (fixed parameters)
Fit an RBF kernel to the training set with parameters `gamma` and `cost` both set to 1. Use the model to predict on the test set. 

```{r}
fit_svm <- svm(Class ~ ., data=df_train, gamma=1, cost=1, kernel="radial")

pred_svm_train <- predict(fit_svm, df_train)
pred_svm_test <- predict(fit_svm, df_test)
```

## 2. Confusion Matrix
Look at the confusion matricies for both the training and testing predictions from the above model. What do you notice about the predictions from this model?

### Training data
```{r}
conf_train <- confusionMatrix(pred_svm_train, df_train$Class)
pander(conf_train$table)
pander(conf_train$overall)
```

Looking at the confusion matrix on the training set shows, that the accuracy is at 100%. That means that the model classifies all 4 classes 100% correctly. This is an indication that there is overfitting happening. In order to check this lets look at the test data.

### Test data
```{r}
conf_test <- confusionMatrix(pred_svm_test, df_test$Class)
pander(conf_test$table)
pander(conf_test$overall)

rm(fit_svm, pred_svm_test, pred_svm_train)
```

The above table shows that the accuracy did drop heavealy from 100% to 72.3% which supports the above conclusion of an overfit. Furthermore, all the classes are classified as group 3, i.e., the majority group. This means that the model act as a 'naive' majority group classifier. 

## 3. The $gamma$ parameter for a RBF kernel
For the RBF kernel, make a figure showing the effect of the kernel parameter $\gamma$ on the training and test errors? Consider some values of `gamma` between 0.001 and 0.3. Explain what you are seeing. 

### Gamma simulation
```{r}
# Set values
gamma_values <- seq(0.001, 0.3, length.out=100)
test_errors <- rep(0., length(gamma_values))
train_errors <- rep(0., length(gamma_values))

# Loop through each gamma value
for (i in 1:length(gamma_values)) {
  fit <- svm(Class ~ ., gamma=gamma_values[i], cost=1, data=df_train, kernel='radial')
  pred_train <- predict(fit, newdata=df_train)
  pred_test <- predict(fit, newdata=df_test)
  train_errors[i] <- classError(pred_train, df_train$Class)$errorRate
  test_errors[i] <- classError(pred_test, df_test$Class)$errorRate
}

# Wrap in a dataframe
df_err <- data.frame(gamma_values, test_errors, train_errors)
rm(fit, gamma_values, i, pred_test, pred_train, test_errors, train_errors, conf_test, conf_train)
```

### Output
```{r}
# Calculate best gammma
best_gamma <- df_err$gamma_values[which(df_err$test_errors == min(df_err$test_errors))]

# Print output
cat('Gamma with lowest test error:', best_gamma)
cat('\ntest error:', min(df_err$test_errors))
```

### Visualization
```{r}
# Tidy data 
df_err <- gather(df_err, key=gamma_values)
names(df_err) <- c("gamma_values", "data", "error")

# Plot
ggplot(data=df_err, aes(x=gamma_values, y=error, color=data)) +
  geom_point(size=0.9) + 
  geom_line() +
  scale_colour_manual(values=c("darkred","black")) +
  ylab("Error rate") +
  xlab("Gamma value") +
  ggtitle("Plot I: SVM train and test error rate by gamma value") +
  theme_bw()
```

The above plot shows, that intially, when the $gamma$ value is increased, the error rate on the training as well as the test data decreases. However, after a *threshold* value where $gamma$ equals 0.007 the test error starts to increase up to a plateau of arround 30% while the training error continuous to decend until it reaches 0%. That means that after the *threshold* $gamma$ the svm starts overfitting the data. 

Technically speaking, Gamma is the free parameter of the Gaussian radial basis function (RBF).

$K(x_i, x_j) = exp(-gamma ||x_i-x_j||^2, gamma >0$

A small $gamma$ means a Gaussian with a large variance so the influence of $x_j$ is more, i.e. if $x_j$ is a support vector, a small gamma implies the class of this support vector will have influence on deciding the class of the vector $x_i$ even if the distance between them is large. If gamma is large, then variance is small implying the support vector does not have wide-spread influence, i.e., a large $gamma$ leads to high bias and low variance models, and vice-versa.

On the available data, with costs set to 1, a small $gamma$ appears to perform better on out-of-sample data. 

## 4. The $cost$ parameter
For the RBF kernel, make a figure showing the effect of the `cost` parameter on the training and test errors? Consider some values of `cost` in the range of 0.1 to 20. Explain what you are seeing. 

### Cost simulation
```{r}
# Initiate list with costs and lists to store error rates
cost_values <- seq(0.1, 20, length.out=50)
test_errors <- rep(0., length(cost_values))
train_errors <- rep(0., length(cost_values))

# Loop through each cost value, fitting an svm and calculating training and test error artes
for (i in 1:length(cost_values)) {
  fit <- svm(Class ~ ., gamma=best_gamma, cost=cost_values[i], data=df_train, kernel='radial')
  pred_train <- predict(fit, newdata=df_train)
  pred_test <- predict(fit, newdata=df_test)
  train_errors[i] <- classError(pred_train, df_train$Class)$errorRate
  test_errors[i] <- classError(pred_test, df_test$Class)$errorRate
}

df_err <- data.frame(cost_values, test_errors, train_errors)
rm(fit, cost_values, i, pred_test, pred_train, test_errors, train_errors)
```

### Output
```{r}
# Calculate best gammma
best_cost <- df_err$cost_values[which(df_err$test_errors == min(df_err$test_errors))]

# Print output
cat('cost with lowest test error:', best_cost)
cat('\ntest error:', min(df_err$test_errors))
```

### Visualization
```{r}
# Tidy data 
df_err <- gather(df_err, key=cost_values)
names(df_err) <- c("cost_values", "data", "error")

# Plot
ggplot(data=df_err, aes(x=cost_values, y=error, color=data)) +
  geom_point(size=0.9) + 
  geom_line() +
  scale_colour_manual(values=c("darkred","black")) +
  ylab("Error rate") +
  xlab("Cost value") +
  ggtitle("Plot II: SVM train and test error rate by cost value") +
  theme_bw()
```

Similar to plot I, the error rate decreases in both the training as well as the test data. At the *threshold* level of 2.13 the test error plateaus and starts increasing at around 8. The training error continuous decreassing up to zero. 

In general, a standard SVM seeks to find a margin that separates all positive and negative examples. As this can lead to poorly fit models *soft margins* are used which allows some examples to be *ignored* or placed on the wrong side of the margin. *C* is the parameter for the soft margin cost function, which controls the influence of each individual support vector, i.e., trading trading error penalty for model stability.

## 5. SVM Kernel tuning (linear, polynomial, RBF)
Fit SVM models with the linear, polynomial (degree 2) and RBF kernels to the training set, and report the misclassification error on the test set for each model. Do not forget to tune all relevant parameters using 5-fold cross-validation on the training set (tuning may take a while!).

### Parameter values
The following parameter values are informed from the allready performed optimization in 1-4. The gamma vector is only for the rbf kernel. 
```{r}
# Create a cost vector
cost_vect <- seq(0.001, 5, length.out=20)

# Create a gamma vector (for RBF only)
gamma_vect <- seq(0.0001, 0.2, length.out=20)
rm(best_cost, best_gamma, df_err)
```

### Linear Kernel
```{r}
# 5-fold crossvalidation 
set.seed(123)
fit_lin_cv <- tune(svm,
                   Class ~ .,
                   data=df_train,
                   kernel="linear",
                   tunecontrol=tune.control(sampling="cross", cross=5),
                   ranges=list(cost=cost_vect)) 

# Build model
fit_lin <- svm(Class ~ ., 
               data=df_train, 
               kernel="linear",
               cost=fit_lin_cv$best.parameters$cost)

# Prediction
pred_lin_test <- predict(fit_lin, df_test)
```

### 2nd degree polynomial kernel
```{r}
# 5-fold crossvalidation to tune the cost 
set.seed(123)
fit_poly_cv <- tune(svm,
                    Class ~ .,
                    data=df_train,
                    kernel="polynomial", 
                    degree=2,
                    tunecontrol=tune.control(sampling="cross", cross=5),
                    ranges=list(cost=cost_vect)) 

# Build model
fit_poly <- svm(Class ~ ., 
                data=df_train, 
                kernel="polynomial",
                cost=fit_poly_cv$best.parameters$cost)

# Prediction
pred_poly_test <- predict(fit_poly, df_test)
```

### RBF kernel
```{r}
# 5-fold crossvalidation to tune the cost 
set.seed(123)
fit_radial_cv <- tune(svm,
                      Class ~ .,
                      data=df_train,
                      kernel="radial", 
                      tunecontrol=tune.control(sampling="cross", cross=5),
                      ranges=list(cost=cost_vect,
                                  gamma=gamma_vect)) 

# Build model
fit_radial <- svm(Class ~ ., 
                  data=df_train, 
                  kernel="radial",
                  cost=fit_radial_cv$best.parameters$cost,
                  gamma=fit_radial_cv$best.parameters$gamma)

# Prediction
pred_radial_test <- predict(fit_radial, df_test)
```

### Data preprocessing for visualization
```{r}
# Radial kernel, pick cost with optimal gamma
df_rad_cv <- fit_radial_cv$performances
df_opt <- df_rad_cv[1, 1:3]
for(i in 2:length(cost_vect)){
  df_rad_cv_in <- df_rad_cv[df_rad_cv$cost == cost_vect[i], 1:3]
  df_opt[i, "cost"] <- df_rad_cv_in$cost[df_rad_cv_in$error == min(df_rad_cv_in$error)]
  df_opt[i, "gamma"] <- df_rad_cv_in$gamma[df_rad_cv_in$error == min(df_rad_cv_in$error)]
  df_opt[i, "error"] <- df_rad_cv_in$error[df_rad_cv_in$error == min(df_rad_cv_in$error)]
}

# Tidy dataframe
df_cv <- data.frame(cost=cost_vect,
                    linear_cv_error=fit_lin_cv$performances$error,
                    poly_cv_error=fit_poly_cv$performances$error,
                    radial_cv_error=df_opt$error)
df_cv <- gather(df_cv, key=cost)
names(df_cv) <- c("cost", "model", "cv_error")

rm(df_rad_cv, df_rad_cv_in, i)
```

### Visualization
```{r}
ggplot(data=df_cv, aes(x=cost, y=cv_error, color=model)) +
  geom_point(size=0.9) + 
  geom_line() +
  ggtitle("Plot III: Crossvalidation - Cost vs. Error") +
  scale_colour_manual(values=c("black","darkred","darkgrey")) +
  ylab("Cross-validation error") +
  xlab("Cost (for radial kernel with optimal gamma)") +
  theme_bw()
```

The above plot shows the cross-validation error rate compared with the cost parameter. For the radial kernel the optimal $gamma$ value is used. 

6. What is the best model in terms of testing accuracy? How does your final model compare with a naive classifier that predicts the most common class (3) on all points?

### Testing
```{r}
# Confustion matrix (testing)
conf_lin <- confusionMatrix(pred_lin_test, df_test$Class)
conf_poly <- confusionMatrix(pred_poly_test, df_test$Class)
conf_radial <- confusionMatrix(pred_radial_test, df_test$Class)

# Output table
pander(data.frame("Linear_accuracy"=conf_lin$overall[1],
                  "Poly_accuracy"=conf_poly$overall[1],
                  "Radial_accuracy"=conf_radial$overall[1]))
```

I looks like the linear as well as radial model achieve very high accuracy rates on the test data. The radial model appears to beat the linear model by a few decimal points. However, taking the tuning time for this relativly small dataset into account, the linear kernel would be the prefered choice for modelling on bigger data. 

### Naive classifier accuracy
Building a majority class naive classifier. The result is the same as the model build in part 2 above.
```{r}
pred_base <- rep(3, nrow(df_test))
cat('Accuracy: ', 1- classError(pred_base, df_test$Class)$errorRate)
```

All SVM model perform better than the naive classifier (most common class model). The recommended model for this data set is a SVM with radial kernel, for larger data with similar characteristics a model with a linear kernel might be a good choice. 

# Problem 2: Return of the Bayesian Hierarchical Model

We're going to continue working with the dataset introduced in Homework 3 about contraceptive usage by 1934 Bangladeshi women. The data are in `dataset_2.txt` which is now a merge of the training and test data that appeared in Homework 2.

In order to focus on the benefits of Hierarchical Modeling we're going to consider a model with only one covariate (and intercept term). 

1. Fit the following three models

	(a) Pooled Model: a single logistic regression for `contraceptive_use` as a function of `living.children`.  Do not include `district` information.  You should use the `glm` function to fit this model. Interpret the estimated model.
	(b) Unpooled Model: a model that instead fits a separate logistic regression for each `district`.  Use the `glm` function to this model.  *Hint*  The separate logistic regression models can be fit using one application of `glm` by having the model formula be `contraceptive_use ~ -1 + living.children * as.factor(district)`.   Explain why this model formula is accomplishing the task of fitting separate models per district.  Examine the summary output of the fitted model.  Briefly explain the reason for many of the `NA` estimates of the coefficients.
	(c) Bayesian Hierarchical Logistic Model: a Bayesian hierarchical logistic regression model with `district` as the grouping variable.  Use the `MCMChlogit` function in the `MCMCpack` library using arguments similar to the reaction time model in the lecture notes.  Make sure that both coefficients of the linear predictor are assumed to vary by `district` in the model specification.  Describe briefly in words how the results of this model are different from the pooled and unpooled models of parts (a) and (b).


2. In class we discussed that one of the benefits of using Bayesian hierarchical models is that it naturally shares information across the groupings. In this case, information is shared across districts. This is generally known as shrinkage. To explore the degree of shrinkage, we are going to compare coefficients across models and districts based on your results from part 1 above.

	(a) Create a single figure that shows the estimated coefficient to `living.children` as a function of district in each of the three models above.  The horizontal axis should be the districts, and the vertical axis should be the estimated coefficient value (generally three estimated coefficients at each district corresponding to the three models).  Make sure that the points plotted for each model are distinct (different colors and/or plotting characters), and that you create a legend identifying the model-specific points on the figure.  You may want to consider adjusting the vertical axis if some estimated coefficients are so large (positively or negatively) that they obscure the general pattern of the bulk of points. Be sure to explain your decision.
	
	(b) Write a short summary (300 words or less) that interprets the graph from part (a).  Pay particular attention to the relationship between the coefficients within each district, and how or whether the number of observations within each district plays a role in the relationship.  You may speculate on the reasons for what you are seeing. 


3. Another benefit of shrinkage is how it affects probability estimates (recall the lucky, drunk friend from lecture whose classical estimate for the probability of guessing correctly was 100%). Extract the estimated probabilities from each model applied to the training data.  That is, for the pooled and unpooled analyses, use the `predict` function applied to the fitted object, using the argument `type="response"`.  For the hierarchical model, the `$theta.pred` component of the fitted model contains the estimated probabilities.
	
	(a) Plot histograms of the vectors of probability estimates for each model separately.  Make sure you standardize the horizontal axis so that the scales are the same.  How does the distribution of estimated probabilities compare across the three models?
	(b) Create a scatter plot comparing predicted values from Unpooled and Hierarchical Models, making sure that the scale of the horizontal and vertical axes are the same, and that the plotting region is square rather than rectangular. Include on the plot the line $y=x$ (why do you think this is a useful line to superimpose?).  Briefly interpret the relationship between the probability estimates for these two models.  Are there particular features of the plot that highlight the intended benefits of using a hierarchical model over the unpooled analysis?  Briefly explain.


# Problem 3: AWS Preparation

In prepartion for the upcoming Spark and Deep Learning modules, submit your AWS
account information. This should have been created in Homework 0. We need specifically:

* The email address associated with your AWS account
* The email address associated with your Harvard ID, if different from above
* Your AWS ID. This should be a 10 digit number. ([Instructions](http://docs.aws.amazon.com/IAM/latest/UserGuide/console_account-alias.html))

We need this information to enable GPU capable compute instances.

```{r child = 'credentials/aws_account_info.Rmd'}
```