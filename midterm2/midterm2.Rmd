---
  title: |
  | Midterm 2: Clustering & SVM's
author: "Tim Hagmann"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output:
pdf_document:
toc: yes
html_document:
css: css/styles.css
highlight: tango
theme: flatly
toc: yes
toc_float: yes
word_document:
toc: yes
linkcolor: blue
subtitle: |
| Advanced Topics in Data Science II
| Harvard University, Spring 2017
affiliation: Harvard University
urlcolor: blue
---

```{r, echo = FALSE}
set.seed(109) # Set seed for random number generator
```

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo=TRUE)
```

```{r global_options, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE,
tidy.opts=list(width.cutoff=60), fig.pos='H',
fig.align='center')
```

# Introduction

In this exam we're asking you to work with measurements of genetic expression for patients with two related forms of cancer: Acute Lymphoblastic Leukemia (ALL) and Acute Myeloid Leukemia (AML). We ask you to perform two general tasks: (1) Cluster the patients based only on their provided genetic expression measurements and (2) classify samples as either ALL or AML using Support Vector Machines.

In the file `MT2_data.csv`, you are provided a data set containing information about a set of 72 different tissue samples. The data have already been split into training and testing when considering the SVM analyses, as the first column indicates. The first 34 samples will be saved for testing while the remaining 38 will be used for training. Columns 2-4 contain the following general information about the sample:
  
- ALL.AML: Whether the patient had AML or ALL.
- BM.PB: Whether the sample was taken from bone marrow or from peripheral blood.
- Gender: The gender of the patient the sample was obtained from.

Note that some of the samples have missing information in these columns. Keep this in mind when conducting some of the analyses below. The remaining columns contain expression measurements for 107 genes. You should treat these as the features. The genes have been pre-selected from a set of about 7000 that are known to be relevant to, and hopefully predictive of, these types of cancers.

# Problem 1: Clustering [60 points]

For the following, **you should use all 72 samples** -- you will only use the genetic information found in columns 5-111 of the dataset. The following questions are about performing cluster analysis of the samples using only genetic data (not columns 2-4). 


```{r init, message=FALSE, warning=FALSE}
## Options
options(scipen = 10)                          # Disable scientific notation
update_package <- FALSE                       # Use old status of packages

## Init files (always execute, eta: 10s)
source("scripts/01_init.R")                   # Helper functions to load packages
source("scripts/02_packages.R")               # Load all necessary packages
source("scripts/03_functions.R")              # Load project specific functions
```

### Load the data
```{r, message=FALSE}
## Read data
df_genes <- data.frame(read_csv("data/MT2_data.csv"))

# Reformat
names(df_genes) <- tolower(names(df_genes))

# Select
df_clust <- df_genes[, 5:111]
```

There are 72 observations and 111 variables present. 107 out of the 111 are genetic expression values. The rest is described in the intro text to this excercise. 

## (a) (10 points)
Standardize the gene expression values, and compute the Euclidean distance between each pair of genes. Apply multi-dimensional scaling to the pair-wise distances, and generate a scatter plot of genes in two dimension. By visual inspection, into how many groups do the genes cluster?  If you were to apply principal components analysis to the standardized data and then plot the first two principal components, how do you think the graph would differ? Briefly justify. (you do not need to perform this latter plot)

### Rescaling
Rescale the data, and compute the Euclidean distance between each pair of states. Generate a heat map of the pair-wise distances.

```{r}
dist_euclidean <- daisy(df_clust, metric="euclidean", stand=TRUE)
print("Plot I: Heat map of the pair-wise distances")
fviz_dist(dist_euclidean,
          gradient=list(low="black", mid="white", high="darkred"),
          lab_size=7)
```

The above plot shows, that there are different Clusters present. Those are marked trough the red color.

### Apply MDS
Apply multi-dimensional scaling to the pair-wise distances, and generate a scatter plot of the genes in two dimension.

### Calculate multi-dimensional scaling (MDS)
```{r}
vec_mds <- cmdscale(dist_euclidean)
colnames(vec_mds) <- c("X1", "X2")
```

#### Visualize
```{r warning=FALSE}
ggplot(data=vec_mds, aes(x=X1, y=X2)) +
  geom_point() +
  geom_point(size=0.9) + 
  geom_density2d(color="darkred") +
  ggtitle("Plot II: Multi-Dimensional Scaling") +
  xlim(-14, 15) +
  ylim(-14, 13) +
  theme_bw()
```

The above conture plot identifies only one cluster at the point (0, 0), however to the naked eye there is a second cluster visible arround the point (5, 2). Furthermore, a outlier is visible in the bottom part of the plot. This indicates that there might be more clusters than the one visualized above.

### PCA plot
I would assume that the 2D dimension plot and the PCA plot would both look similar, but reflected acros 0 on the PC1/Dim1 axis. Thus, MDS and PCA are probably not at the same level. That might show itself in beeing on the opposite side of each other. 

## (b) (10 points)
Apply **Partitioning around medoids** (PAM) to the data, selecting the optimal number of clusters based on the Gap, Elbow and Silhouette statistics -- if the they disagree, select the largest number of clusters indicated by the statistics. Summarize the results of clustering using a principal components plot, and comment on the quality of clustering using a Silhouette diagnostic plot.

### Partitioning around medoids (PAM)
```{r}
gapstat_pam <- clusGap(scale(df_clust),FUN=pam, K.max=10, B=500)
```

### Print output
```{r}
print(gapstat_pam, method = "Tibs2001SEmax")
```

The Tibshirani criterion has the idea that for a particular choice of K clusters, the total within cluster variation is compared to the expected within-cluster variation. According to this creterion two clusters is optimal.

### PAM gap statistic
```{r}
# Calculation
nbc_clust_gap <- fviz_nbclust(scale(df_clust), pam,
                              method="gap_stat", linecolor="darkred",
                              k.max=10)

# Visualization
nbc_clust_gap + ggtitle("Plot III: PAM clustering") 
```

The above plot shows, that the optimal number of clusters according to the gap statistic is 2 cluster. 

### PAM silouhette
```{r}
# Calculation
nbc_clust_sil <- fviz_nbclust(scale(df_clust), pam,
                              method="silhouette", linecolor="darkred",
                              k.max=10)

# Visualization
nbc_clust_sil + ggtitle("Plot IV: PAM clustering") 
```

According to the silhouette plot above the optimal number of clusters is again at 2. 

### Summary
The different methods for the pam algorithm do agree on the optimal amount of clusters. The Gap Stat indicates 2 clusters and the silouhette statistic also 2 clusters.

### Calculate
The next thing is to summarize the results of clustering using a principal components plot.
```{r}
# Options
opt_clust <- 2
sel_pam <- pam(scale(df_clust), opt_clust)

# Calculation
fviz_clust_pam <- fviz_cluster(sel_pam,
                               data=scale(df_clust),
                               linecolor="darkred",
                               labelsize=8)
```

### Visualize PAM
```{r}
fviz_clust_pam + 
  ggtitle("Plot V: PC PAM clustering") +
  theme_bw()
```
The above principal component plot visualizing the two cluster accross the first and second dimension gives a nice picture about the cluster line. There appears to be a small overlapp at the 25/48 point genes.

### Missclassified states
```{r}
# Calculate
sil_pam <- silhouette(sel_pam)[, 3]

# Print missclassified genes
print("PAM misclassified genes:")
print(names(which(sil_pam < 0)))
```

### PAM silhouette
```{r}
fviz_silhouette(silhouette(sel_pam),
                main="Plot VI: Silhouette plot for PAM clustering") +
  theme_bw() +
  scale_fill_manual(values=c("black", "darkred")) +
  scale_color_manual(values=c("white", "white")) +
  theme(axis.text.x=element_text(angle=90, hjust=1, vjust=0.25))
```

The above plot shows, that the PAM algorithm probably misslassified the genes 27, 70, 62, 25, 23 and 6 into the wrong clusters. The first cluster appears to be the stronger of the two with the majority of its point lying well above the dotted line. The case for cluster two appears to be weaker as the majority is either very likely to be missclassfied and and also lying below the dotted line.

## (c) (10 points)
Apply **Agglomerative clustering** (AGNES) with Ward's method to the data. Summarize the results using a dendrogram. Determine the optimal number of clusters in a similar way as in (b), and add rectangles to the dendrograms sectioning off clusters.  Comment on the ways (if any) the results of PAM differ from those of AGNES.

## Agglomerative clustering

```{r}
gapstat_agnes <- clusGap(scale(df_clust),
FUN=agnes.reformat,
K.max=10, B=500)
print(gapstat_agnes, method = "Tibs2001SEmax")
```

According to the Tibshirani metric the optimal amount of clusters is 1. However, in order to visualize a sensible amount of clusters on the dendogram, 2 is chosen. 

## Ward method
```{r}
sel_agnes <- agnes(scale(df_clust),
method="ward",
stand=T)

pltree(agnes(df_clust), cex=0.5, hang=-0.01, main="Plot VII: Dendogram of agnes")
rect.hclust(sel_agnes, k=2, border="darkred")
```

```{r}
table(agnes(df_clust)$order[43:length(agnes(df_clust)$order)] %in%
as.numeric(names(sil_pam[52:length(sil_pam)])))
```

The dendogram for the AGNES clustering shows the two selected clusters. There appears to be some overlapp between the two clusters. However, a siceable number is classified into a different cluster than seen before by the PAM algorithm. 

### Calculations
```{r}
out_agnes <- as.integer((agnes.reformat(scale(df_clust), 2))[[1]])
sil_agnes <- silhouette(out_agnes, dist(scale(df_clust)))[, 3]
index_agnes_neg_sil <- which(sil_agnes < 0)
print("Agnes Misclassified Genes:")
print(row.names(df_clust)[index_agnes_neg_sil])
```

### Visualization
```{r}
fviz_silhouette(silhouette(out_agnes, dist(scale(df_clust))),
main="Plot VIII: Silhouette plot for AGNES clustering") +
theme_bw() +
scale_fill_manual(values=c("black", "darkred")) +
scale_color_manual(values=c("white", "white")) +
theme(axis.text.x=element_text(angle=90, hjust=1, vjust=0.25))
```

The silhouette plot shows that the following genes are probabily missclassified: 2, 3, 21, 23, 25, 27, 38, 41, 42, 43, 61, 62, 63, 66 into cluster 1. Cluster one appears to be rather solit with most of the points beeing classified above the dotted line. However, as seen before, the second cluster appears to be less stable with most of the points beeing below the dotted line.

As already commented upon, both clustering alorighm appear to choose a more stable cluster numer one and a less stable cluster number two. There appears to be some overlapp between the two clustering methods, nevertheless, a sizeable amount of genes are classified into different clusters between the two algorithms.

## (d) (10 points)
Apply **Fuzzy clustering** (FANNY) to the data, determining the optimal number of clusters as in (b). Summarize the results using both a principal components plot, and a correlation plot of the cluster membership weights.  Based on the cluster membership weights, do you think it makes sense to consider summarizing the results using a principal components plot?  Briefly justify.

### Fuzzy Clustering
### Gap statistics fanny
```{r, warning=FALSE}
gapstat_fanny <- clusGap(scale(df_clust),
FUN=fanny,
K.max=10,
B=500)
print(gapstat_fanny, method = "Tibs2001SEmax")
```

According to the Tibshirani metric the optimal amount of clusters is 1. However, in order to visualize a sensible amount of clusters, 2 is chosen. 

### Correlation plot for fanny
```{r}
corrplot(sel_fanny$membership,
is.corr=FALSE,
tl.cex=0.5, tl.col="darkred", cl.cex=0.5)
```

For the case of fuzzy clustering, it appears that the correlation plot indicates difficulties distinguising between the two clusters. Creating a principal components plot would visualize this sitation. It might be helpfull in further indicating which are bordeline cases.

### Visualization
```{r}
sel_fanny <- fanny(df_clust, 2)
fviz_cluster(sel_fanny,
data=scale(df_clust),
main="Plot XIX: Fuzzy Clustering",
labelsize=8) +
theme_bw() +
xlim(-5, 5) +
ylim(-3, 7)
```

As can bee seen above, the genes, 41, 2, 44, 45, 4, 16, 40, 3, 48, 52 as well as 56, 1, 19, 59, 14 are borderline cases. This could already be obseverd in the correlation plot above. The fuzzy algorithm appears to have difficulties disthinguising between the clusters.

### Fanny silhouette plot
```{r}
# Calculations
sil_fanny <- silhouette(sel_fanny)[, 3]
print("Fuzzy misclassified states:")
print(names(which(sil_fanny < 0)))
```

### Visualization
```{r}
fviz_silhouette(silhouette(sel_fanny),
main="Plot XX: Silhouette plot for fuzzy clustering") +
theme_bw() +
scale_fill_manual(values=c("black", "darkred")) +
scale_color_manual(values=c("white", "white")) +
theme(axis.text.x=element_text(angle=90, hjust=1, vjust=0.25))
```

The above plot shows, that there is an indication of one missclassified case. However, in both clusters a lot of genes are lying below the dotted line. This further shows the problem the fuzzy algorithm appears to be having in distinguising between the two clusters. 

### (e) (20 points)
For the clusters found in parts (b)-(d), select just one of the clusterings, preferably with the largest number of clusters. For this clustering, what proportion of each cluster are ALL (Acute Lympohblastic Leukemia) samples? In each cluster, what proportion are samples belonging to female subjects? In each cluster, what proportion of the samples were taken from bone marrow as opposed to peripheral blood? What, if anything, does this analysis imply about the clusters you discovered?

### Prepare data
No clustering algorithm selects more than 2 clusters. Out of all the applied methods the PAM appears to be the most promising. See (b)-(d) for the details. This is why this algorithm is chosen for the further analyis. First we're attaching the clustering to the data.

```{r}
df_clust2 <- df_genes
df_clust2$cluster <- sel_pam$clustering
```

### Calculate all.aml frequency
```{r}
tabl_all.aml <- table(df_clust2$cluster, df_clust2$all.aml)
print("Frequency/Counts table:")
pander(tabl_all.aml)              
print("Proportion table I (Colums):")
pander(round(prop.table(tabl_all.aml, 2), 3))
print("Proportion table II (Rows):")
pander(round(prop.table(tabl_all.aml, 1), 3))
print(paste0("Accuracy: ",
             round(sum(diag(tabl_all.aml))/sum(tabl_all.aml) * 100, 1),
             "%"))
```
The dataset includes 47 ALL and 25 AML samples. Looking at the different clusters shows, that cluster one comprises 44 out of all 47 (93.6%) ALL samples and cluster two 18 out of 25 (72%) AML samples. Cluster one has 51 samples, out of that 86.3% are ALL and 13.7% are AML samples. Cluster two has 21 samples, out of that 14.3% are ALL and 85.7% are AML (Answer to question 1).

Using the PAM clusters as predictors for the ALL and AML leads to an overall accuracy rate of 86.1%. This means the PAM algorithm more or less clusters around the ALL and AML samples and is a good predictor for them.

### Calculate gender frequency
```{r}
df_clust2$gender[is.na(df_clust2$gender)] <- "missing"
tabl_gender <- table(df_clust2$cluster, df_clust2$gender)
print("Frequency/Counts table:")
pander(tabl_gender)              
print("Proportion table I (Colums):")
pander(round(prop.table(tabl_gender, 2), 3))
print("Proportion table II (Rows):")
pander(round(prop.table(tabl_gender, 1), 3))
print(paste0("Accuracy: ",
             round(sum(diag(tabl_gender))/sum(tabl_gender) * 100, 1),
             "%"))
```

There are a lot of missing values present in the gender variable. This makes an analyis of the gender distribution in each cluster more difficult. As seen before, cluster 1 counts 51 samples, out of that 41.2% are female, 41.2% are male and 17.6% are missing values. cluster 2 counts 21 samples, out of that 9.5% are female, 23.8% are male and 66.7% are missing values. (Answer to question 2) 

Overall cluster one has 91.3% of all females, 80.8% of all males and 39.1% of all missing values. Cluster 2 has 8.7% of all females, 19.2% of all males and 60.9% of all missing values.

The overall accuracy rate when using the clusters for predictors for the three categories (female, male, missing) is at 36.1% which is only slightly above chance. The PAM cluster algorithm doesn't appear to have clustered around the gender of the participants.

### Calculate bm.pb frequency
```{r}
tabl_bm.pb <- table(df_clust2$cluster, df_clust2$bm.pb)
print("Frequency/Counts table:")
pander(tabl_bm.pb)              
print("Proportion table I (Colums):")
pander(round(prop.table(tabl_bm.pb, 2), 3))
print("Proportion table II (Rows):")
pander(round(prop.table(tabl_bm.pb, 1), 3))
print(paste0("Accuracy: ",
             round(sum(diag(tabl_bm.pb))/sum(tabl_bm.pb) * 100, 1),
             "%"))
```

Looking at the bm.pb variable it appears that roughly 2/3 of all BM and BP data is in cluster one and 1/3 of all BM and BP is in cluster 2. Using the clusters as a predictor would lead to an overall accuracy rate of 65.3%. Even when beeing well above chance (50%) it doesn't appear that the PAM clusters are good predictors for the two different sampling methods.

In cluster 1 the proportion of BM 86.3% vs. 13.7% PB and in cluster 2 it is 85.7% BM vs. 14.3% BP. (Answer to question 3) 

**Summary:** The PAM algorithm appears to cluster more or less around the ALL and AML samples and is a good predictor for them. This isn't the case for the gender or bm.pb variable. (Answer to question 4)

# Problem 2: Classification [40 points]

For the following problem, we will not be using the general information about the sample due to missing values. Subset the columns keeping only the ALL.AML and the 107 genetic expression values. Then split the samples into two datasets, one for training and one for testing, according to the indicator in the first column. There should be 38 samples for training and 34 for testing. 

### Data selection
```{r}
df_train <- df_genes[df_genes$train.test == "Train", c(2, 5:length(df_genes))]
df_test <- df_genes[df_genes$train.test == "Test", c(2, 5:length(df_genes))]

df_train$all.aml <- as.factor(df_train$all.aml)
df_test$all.aml <- as.factor(df_test$all.aml)
```

The following questions essentially  create a diagnostic tool for predicting whether a new patient likely has Acute Lymphoblastic Leukemia or Acute Myeloid Leukemia based only on their genetic expression values.

## (a) (15 points)
Fit two SVM models with linear and RBF kernels to the training set, and report the classification accuracy of the fitted models on the test set. Explain in words how linear and RBF kernels differ as part of the SVM. In tuning your SVMs, consider some values of `cost` in the range of 1e-5 to 1 for the linear kernel and for the RBF kernel, `cost` in the range of 0.5 to 20  and `gamma` between 1e-6 and 1. Explain what you are seeing. 

### Train a svm with 5 fold cv
```{r}
set.seed(109)
# Set parameters
costs_lin <- seq(from=1e-5, to=1, by=0.05)
costs_rbf <- seq(from=0.5, to=20, by=0.75)
gammas <- seq(from=1e-6, to=1, by=0.05)

# Fit model
fit_svm_cv_lin <- tune(svm, all.aml ~ .,
data=df_train,
tunecontrol=tune.control(cross=5),
ranges=list(cost=costs_lin,
kernel='linear'))

fit_svm_cv_rad <- tune(svm, all.aml ~ .,
data=df_train,
tunecontrol=tune.control(cross=5),
ranges=list(cost=costs_rbf, gamma=gammas,
kernel='radial'))
```

### Predict accuracy
```{r}
# Best SVM
pred_svm_lin <- predict(fit_svm_cv_lin$best.model, df_test)
pred_svm_rad <- predict(fit_svm_cv_rad$best.model, df_test)

# Confusion matrix
cm_svm_lin <- confusionMatrix(table(pred_svm_lin, df_test$all.aml))
cm_svm_rad <- confusionMatrix(table(pred_svm_rad, df_test$all.aml))

# Output
pander(cm_svm_lin$table)
pander(cm_svm_lin$overall)
pander(cm_svm_rad$table)
pander(cm_svm_rad$overall)
```

It appears that the linear SVM kernel is much better in detecting AML cases. The confusion matrices show, that the radial kernel classifies all observations into the ALL class. This results in an overall test accuracy of 58.8%. The Linear kernel on the other hand reaches an overall test accuracy of 88.2%. The linear kernel appears to be better able in detecting the AML class. It detects 10 AML cases and only missclassifies 4 cases as ALL.

(b) (10 points) Apply principal component analysis (PCA) to the genetic expression values in the training set, and retain the minimal number of PCs that capture at least 90% of the variance in the data. How does the number of PCs identified compare with the total number of gene expression values?  Apply to the test data the rotation that resulted in the PCs in the training data, and keep the same set of PCs.


```{r}
# Fit the model
fit_pca <- prcomp(df_train[, 2:length(df_train)], scale=TRUE, center=TRUE)

# Get the amount on variance
pca_variance <- fit_pca$sdev^2/sum(fit_pca$sdev^2)
```

### Calculate the pc's retaining 90% of the variation
```{r}
top_pca <- min(which(cumsum(pca_variance) >= 0.90))
print(max(top_pca))
```

Instead of using 108 Columns only 23 are sufficient in order to retain 90% of the variability in the data. This helps migigating the *curse of dimensionality* problem in the data. The curse of dimensionality refers to the phenomena that arise when analyzing data in a very high-dimensional spaces. The problem is that when the dimensionality increases, the volume of the space increases so fast that the available data become sparse. More specifically, the amount of data needed to support the result grows exponentially with the dimensionality. Furthermore, it helps in decreasing the calculation time when performing various optimization methods.

### Transforming the test and training data
Next we're applying to all the data (train & test) the rotation that resulted in the PCs in the training data, and keep the same set of PCs.

```{r}
pca_90_vectors <- fit_pca$rotation[, 1:top_pca]

# Rescale
df_pca_train <- scale(df_train[, 2:length(df_train)],
center=fit_pca$center,
scale=fit_pca$scale)

df_pca_test <- scale(df_test[, 2:length(df_test)],
center=fit_pca$center,
scale=fit_pca$scale)

df_pca_train <- df_pca_train %*% fit_pca$rotation[, 1:top_pca]
df_pca_test <- df_pca_test %*% fit_pca$rotation[, 1:top_pca]

# Put together in df
df_pca_train <- data.frame(all.aml=df_train$all.aml, df_pca_train)
df_pca_test <- data.frame(all.aml=df_test$all.aml, df_pca_test)
```

(c) (15 points) Fit a SVM model with linear and RBF kernels to the reduced training set, 
and report the classification accuracy of the fitted models on the reduced test set. Do not forget to tune the regularization and kernel parameters by cross-validation. How does the test accuracies compare with the previous models from part (a)? What does this convey? *Hint*: You may use similar ranges for tuning as in part (a), but for the RBF kernel you may need to try even larger values of `cost`, i.e. in the range of 0.5 to 40. 

```{r}
# Fit model
fit_svm_cv_lin_red <- tune(svm, all.aml ~ .,
data=df_pca_train,
tunecontrol=tune.control(cross=5),
ranges=list(cost=costs_lin,
kernel='linear'))

fit_svm_cv_rad_red <- tune(svm, all.aml ~ .,
data=df_pca_train,
tunecontrol=tune.control(cross=5),
ranges=list(cost=costs_rbf, gamma=gammas,
kernel='radial'))
```

### Predict accuracy
```{r}
# Best SVM
pred_svm_lin_red <- predict(fit_svm_cv_lin_red$best.model, df_pca_test)
pred_svm_rad_red <- predict(fit_svm_cv_rad_red$best.model, df_pca_test)

# Confusion matrix
cm_svm_lin_red <- confusionMatrix(table(pred_svm_lin_red, df_pca_test$all.aml))
cm_svm_rad_red <- confusionMatrix(table(pred_svm_rad_red, df_pca_test$all.aml))

# Output
pander(cm_svm_lin_red$table)
pander(cm_svm_lin_red$overall)
pander(cm_svm_rad_red$table)
pander(cm_svm_rad_red$overall)
```

The above statistics show, that the linear kernel appears to perfom slightly worse than before, one more AML case was missclassified as ALL. This means that the test accuracy decreased to 85.2%. The radial kernel on the other hand performs now better than before. It now reaches an overall accuracy of 79.4% and is able to correctly identify 7 AML cases. Nevertheless, it still performs wore than the linear kernel applied on the untransformed data. Next we're performing some further RBF tuning.

### Tune RBF further
```{r}
set.seed(109)
# Set parameters
costs_rbf2 <- seq(from=0.5, to=40, by=0.5)
gammas2 <- seq(from=1e-6, to=1, by=0.025)

# Fit model
fit_svm_cv_rad_red2 <- tune(svm, all.aml ~ .,
                            data=df_pca_train,
                            tunecontrol=tune.control(cross=5),
                            ranges=list(cost=costs_rbf2, gamma=gammas2,
                                        kernel='radial'))
```

### Predict accuracy
```{r}
# Best SVM
pred_svm_rad_red2 <- predict(fit_svm_cv_rad_red2$best.model, df_pca_test)

# Confusion matrix
cm_svm_rad_red2 <- confusionMatrix(table(pred_svm_rad_red2, df_pca_test$all.aml))

# Output
pander(cm_svm_rad_red2$table)
pander(cm_svm_rad_red2$overall)
```

